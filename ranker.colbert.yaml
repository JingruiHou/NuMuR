# model
seed: 10
bert_pretrained_model: distilbert-base-uncased
bert_trainable: True
dual_encoder: False
batch_size: 32
prediction_batch_size: 32
epochs: 3
distil_epochs: 20
distil_learning_rate: 0.0001
learning_rate: 0.000007
colbert_compression_dim: 768
dropout: 0.0
return_vecs: False

unk_token: 0
query_max_len: 20
target_max_len: 200
model_type: colbert
model_name: ColBERT
from_self.config: True

distillation_margin: 5
distillation_ratio: 1
